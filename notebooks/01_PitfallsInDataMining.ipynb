{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01 - Pitfalls in Data Mining     \n",
    "## CSCI E-96\n",
    "\n",
    "The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences.   \n",
    "\n",
    "A related problem is cutting off a large-scale analysis when a desired relationship is 'found'. This practice of **p-value mining** often leads to unwarranted inferences.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hypothesis Tesing\n",
    "\n",
    "Testing multiple hypothesis in high-dimensional data can be problematic. Exhaustively testing all pairwise relationships between variables in a data set is a commonly used, but generally misleading from of **multiple comparisons**. The chance of finding false significance, using such a **data dredging** approach, can be surprisingly high. \n",
    "\n",
    "In this exercise you will perform multiple comparisons on only 20 **identically distributed independent (iid)** variables. Ideally, such tests should not find significant relationships, but the actual result is quite different. \n",
    "\n",
    "To get started, execute the code in the cell below to load the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from itertools import product, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will apply a t-test to all pairwise combinations of identical Normally distributed variables. In this case, we will create a data set with 20 iid Normal distributions of 1000 samples each. Execute the code in the cell below to find this data and display the mean and variance of each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The means of the columns are\n",
      " [-1.16191649e-01  2.80829317e-02 -1.78516419e-02 -1.44691489e-02\n",
      "  3.03718152e-02  1.20007442e-02 -9.58845606e-05  1.98662580e-03\n",
      "  4.94154934e-02 -4.11640866e-02 -6.32977862e-03 -5.93868192e-02\n",
      " -2.56373595e-02  1.43568791e-02 -1.44725765e-02 -1.37023955e-02\n",
      "  1.80622439e-02  5.87029691e-02 -2.02650514e-02 -1.56346106e-02]\n",
      "\n",
      "The variances of the columns are\n",
      " [0.94834508 1.04744241 1.0258018  0.96977571 1.0089001  1.04113864\n",
      " 1.00657222 0.99192594 1.04713487 1.04329434 1.04023108 0.96791346\n",
      " 1.03706907 1.07179865 1.01431404 1.05060289 1.02054329 0.9686211\n",
      " 1.02810287 0.99521555]\n"
     ]
    }
   ],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(234)\n",
    "normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "print('The means of the columns are\\n', np.mean(normal_vars, axis = 0))\n",
    "print('\\nThe variances of the columns are\\n', np.var(normal_vars, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that means and variances are close to 0.0 and 1.0. As expected, there is not much difference between these variables.\n",
    "\n",
    "How many of these t-tests will show **significance** at the 0.05 cut-off level? There are 380 pairwise combinations, so we expect to find a number of falsely significant test results at this level. To find out, complete and execute the code in the cell below to filter the test results and print those that show significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a hash \n",
    "\n",
    "The goal of this exercise is to compute pairwise hypothesis tests of the differences in means for each of the iid Normal vectors. As an intermediate step you will create a dictionary with the results of these hypothesis tests. The dictionaries store **key-value**, $(K,V)$, pairs. Each key must represent an index for the two vectors used to compute the test statistic. \n",
    "\n",
    "The question is, how can we represent the key for the pair of vectors? One option is to use a dictionary of dictionaries. This approach has a dictionary indexed by the first key, which contains a dictionary indexed by the second key. While, this nested dictionary approach would work, it requires two key look-ups per value. A better approach is to create a hash of the two indexes and use the hash as the key for the dictionary. Using a hash the values in the dictionary can be accessed in a single step with using hashed key.\n",
    "\n",
    "\n",
    "> **Computational Note:** The Python dictionary is an efficient and reasonably scalable **hash table**. The hash function used depends on the type of the key; integer, string, etc. The resulting dictionary of key-value pairs, $(K,V)$, can therefore be access in far less than linear time, often about $O(log(N))$.  \n",
    "\n",
    "If you are not familiar with Python dictionaries you can find a short tutorial [here](https://www.tutorialspoint.com/python_data_structure/python_hash_table.htm), as well as many other places on the web.\n",
    "\n",
    "> **Exercise 1-1:** Given that our space of vectors is actually quite small, just 20, we do not need a sophisticated and scalable hash function. This hashed key will then be used to store and retrieve the values using a Python dictionary, in about $O(log(N))$ time.     \n",
    "\n",
    "> In this exercise you will test a simple hash function and its inverse. Examine the code below and notice that the hash function encodes the two indexes into a single integer by simple additional and multiplication. Division (a slower process) is avoided. Efficiency of the inverse hash function is less important, since it is used less frequently.  \n",
    "\n",
    "> To test this hash, do the following:    \n",
    "> 1. Using the Python [ittertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations) function create all unique pairwise combinations of indexes i and j. The arguments to this function are the indexes to the iid Normal vectors. The iterator is `range(ncolumns)`.    \n",
    "> 2. Within this loop compute the hash, and the inverse hash of the indexes, i, and j.   \n",
    "> 3. On a single line print the following; the values of i and j, the hash key value, and the unhashed values of i and j.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0  j = 1   hash = 8192  after, i= 0  j = 1\n",
      "i = 0  j = 2   hash = 12288  after, i= 0  j = 2\n",
      "i = 0  j = 3   hash = 16384  after, i= 0  j = 3\n",
      "i = 0  j = 4   hash = 20480  after, i= 0  j = 4\n",
      "i = 0  j = 5   hash = 24576  after, i= 0  j = 5\n",
      "i = 0  j = 6   hash = 28672  after, i= 0  j = 6\n",
      "i = 0  j = 7   hash = 32768  after, i= 0  j = 7\n",
      "i = 0  j = 8   hash = 36864  after, i= 0  j = 8\n",
      "i = 0  j = 9   hash = 40960  after, i= 0  j = 9\n",
      "i = 0  j = 10   hash = 45056  after, i= 0  j = 10\n",
      "i = 0  j = 11   hash = 49152  after, i= 0  j = 11\n",
      "i = 0  j = 12   hash = 53248  after, i= 0  j = 12\n",
      "i = 0  j = 13   hash = 57344  after, i= 0  j = 13\n",
      "i = 0  j = 14   hash = 61440  after, i= 0  j = 14\n",
      "i = 0  j = 15   hash = 65536  after, i= 0  j = 15\n",
      "i = 0  j = 16   hash = 69632  after, i= 0  j = 16\n",
      "i = 0  j = 17   hash = 73728  after, i= 0  j = 17\n",
      "i = 0  j = 18   hash = 77824  after, i= 0  j = 18\n",
      "i = 0  j = 19   hash = 81920  after, i= 0  j = 19\n",
      "i = 1  j = 2   hash = 12289  after, i= 1  j = 2\n",
      "i = 1  j = 3   hash = 16385  after, i= 1  j = 3\n",
      "i = 1  j = 4   hash = 20481  after, i= 1  j = 4\n",
      "i = 1  j = 5   hash = 24577  after, i= 1  j = 5\n",
      "i = 1  j = 6   hash = 28673  after, i= 1  j = 6\n",
      "i = 1  j = 7   hash = 32769  after, i= 1  j = 7\n",
      "i = 1  j = 8   hash = 36865  after, i= 1  j = 8\n",
      "i = 1  j = 9   hash = 40961  after, i= 1  j = 9\n",
      "i = 1  j = 10   hash = 45057  after, i= 1  j = 10\n",
      "i = 1  j = 11   hash = 49153  after, i= 1  j = 11\n",
      "i = 1  j = 12   hash = 53249  after, i= 1  j = 12\n",
      "i = 1  j = 13   hash = 57345  after, i= 1  j = 13\n",
      "i = 1  j = 14   hash = 61441  after, i= 1  j = 14\n",
      "i = 1  j = 15   hash = 65537  after, i= 1  j = 15\n",
      "i = 1  j = 16   hash = 69633  after, i= 1  j = 16\n",
      "i = 1  j = 17   hash = 73729  after, i= 1  j = 17\n",
      "i = 1  j = 18   hash = 77825  after, i= 1  j = 18\n",
      "i = 1  j = 19   hash = 81921  after, i= 1  j = 19\n",
      "i = 2  j = 3   hash = 16386  after, i= 2  j = 3\n",
      "i = 2  j = 4   hash = 20482  after, i= 2  j = 4\n",
      "i = 2  j = 5   hash = 24578  after, i= 2  j = 5\n",
      "i = 2  j = 6   hash = 28674  after, i= 2  j = 6\n",
      "i = 2  j = 7   hash = 32770  after, i= 2  j = 7\n",
      "i = 2  j = 8   hash = 36866  after, i= 2  j = 8\n",
      "i = 2  j = 9   hash = 40962  after, i= 2  j = 9\n",
      "i = 2  j = 10   hash = 45058  after, i= 2  j = 10\n",
      "i = 2  j = 11   hash = 49154  after, i= 2  j = 11\n",
      "i = 2  j = 12   hash = 53250  after, i= 2  j = 12\n",
      "i = 2  j = 13   hash = 57346  after, i= 2  j = 13\n",
      "i = 2  j = 14   hash = 61442  after, i= 2  j = 14\n",
      "i = 2  j = 15   hash = 65538  after, i= 2  j = 15\n",
      "i = 2  j = 16   hash = 69634  after, i= 2  j = 16\n",
      "i = 2  j = 17   hash = 73730  after, i= 2  j = 17\n",
      "i = 2  j = 18   hash = 77826  after, i= 2  j = 18\n",
      "i = 2  j = 19   hash = 81922  after, i= 2  j = 19\n",
      "i = 3  j = 4   hash = 20483  after, i= 3  j = 4\n",
      "i = 3  j = 5   hash = 24579  after, i= 3  j = 5\n",
      "i = 3  j = 6   hash = 28675  after, i= 3  j = 6\n",
      "i = 3  j = 7   hash = 32771  after, i= 3  j = 7\n",
      "i = 3  j = 8   hash = 36867  after, i= 3  j = 8\n",
      "i = 3  j = 9   hash = 40963  after, i= 3  j = 9\n",
      "i = 3  j = 10   hash = 45059  after, i= 3  j = 10\n",
      "i = 3  j = 11   hash = 49155  after, i= 3  j = 11\n",
      "i = 3  j = 12   hash = 53251  after, i= 3  j = 12\n",
      "i = 3  j = 13   hash = 57347  after, i= 3  j = 13\n",
      "i = 3  j = 14   hash = 61443  after, i= 3  j = 14\n",
      "i = 3  j = 15   hash = 65539  after, i= 3  j = 15\n",
      "i = 3  j = 16   hash = 69635  after, i= 3  j = 16\n",
      "i = 3  j = 17   hash = 73731  after, i= 3  j = 17\n",
      "i = 3  j = 18   hash = 77827  after, i= 3  j = 18\n",
      "i = 3  j = 19   hash = 81923  after, i= 3  j = 19\n",
      "i = 4  j = 5   hash = 24580  after, i= 4  j = 5\n",
      "i = 4  j = 6   hash = 28676  after, i= 4  j = 6\n",
      "i = 4  j = 7   hash = 32772  after, i= 4  j = 7\n",
      "i = 4  j = 8   hash = 36868  after, i= 4  j = 8\n",
      "i = 4  j = 9   hash = 40964  after, i= 4  j = 9\n",
      "i = 4  j = 10   hash = 45060  after, i= 4  j = 10\n",
      "i = 4  j = 11   hash = 49156  after, i= 4  j = 11\n",
      "i = 4  j = 12   hash = 53252  after, i= 4  j = 12\n",
      "i = 4  j = 13   hash = 57348  after, i= 4  j = 13\n",
      "i = 4  j = 14   hash = 61444  after, i= 4  j = 14\n",
      "i = 4  j = 15   hash = 65540  after, i= 4  j = 15\n",
      "i = 4  j = 16   hash = 69636  after, i= 4  j = 16\n",
      "i = 4  j = 17   hash = 73732  after, i= 4  j = 17\n",
      "i = 4  j = 18   hash = 77828  after, i= 4  j = 18\n",
      "i = 4  j = 19   hash = 81924  after, i= 4  j = 19\n",
      "i = 5  j = 6   hash = 28677  after, i= 5  j = 6\n",
      "i = 5  j = 7   hash = 32773  after, i= 5  j = 7\n",
      "i = 5  j = 8   hash = 36869  after, i= 5  j = 8\n",
      "i = 5  j = 9   hash = 40965  after, i= 5  j = 9\n",
      "i = 5  j = 10   hash = 45061  after, i= 5  j = 10\n",
      "i = 5  j = 11   hash = 49157  after, i= 5  j = 11\n",
      "i = 5  j = 12   hash = 53253  after, i= 5  j = 12\n",
      "i = 5  j = 13   hash = 57349  after, i= 5  j = 13\n",
      "i = 5  j = 14   hash = 61445  after, i= 5  j = 14\n",
      "i = 5  j = 15   hash = 65541  after, i= 5  j = 15\n",
      "i = 5  j = 16   hash = 69637  after, i= 5  j = 16\n",
      "i = 5  j = 17   hash = 73733  after, i= 5  j = 17\n",
      "i = 5  j = 18   hash = 77829  after, i= 5  j = 18\n",
      "i = 5  j = 19   hash = 81925  after, i= 5  j = 19\n",
      "i = 6  j = 7   hash = 32774  after, i= 6  j = 7\n",
      "i = 6  j = 8   hash = 36870  after, i= 6  j = 8\n",
      "i = 6  j = 9   hash = 40966  after, i= 6  j = 9\n",
      "i = 6  j = 10   hash = 45062  after, i= 6  j = 10\n",
      "i = 6  j = 11   hash = 49158  after, i= 6  j = 11\n",
      "i = 6  j = 12   hash = 53254  after, i= 6  j = 12\n",
      "i = 6  j = 13   hash = 57350  after, i= 6  j = 13\n",
      "i = 6  j = 14   hash = 61446  after, i= 6  j = 14\n",
      "i = 6  j = 15   hash = 65542  after, i= 6  j = 15\n",
      "i = 6  j = 16   hash = 69638  after, i= 6  j = 16\n",
      "i = 6  j = 17   hash = 73734  after, i= 6  j = 17\n",
      "i = 6  j = 18   hash = 77830  after, i= 6  j = 18\n",
      "i = 6  j = 19   hash = 81926  after, i= 6  j = 19\n",
      "i = 7  j = 8   hash = 36871  after, i= 7  j = 8\n",
      "i = 7  j = 9   hash = 40967  after, i= 7  j = 9\n",
      "i = 7  j = 10   hash = 45063  after, i= 7  j = 10\n",
      "i = 7  j = 11   hash = 49159  after, i= 7  j = 11\n",
      "i = 7  j = 12   hash = 53255  after, i= 7  j = 12\n",
      "i = 7  j = 13   hash = 57351  after, i= 7  j = 13\n",
      "i = 7  j = 14   hash = 61447  after, i= 7  j = 14\n",
      "i = 7  j = 15   hash = 65543  after, i= 7  j = 15\n",
      "i = 7  j = 16   hash = 69639  after, i= 7  j = 16\n",
      "i = 7  j = 17   hash = 73735  after, i= 7  j = 17\n",
      "i = 7  j = 18   hash = 77831  after, i= 7  j = 18\n",
      "i = 7  j = 19   hash = 81927  after, i= 7  j = 19\n",
      "i = 8  j = 9   hash = 40968  after, i= 8  j = 9\n",
      "i = 8  j = 10   hash = 45064  after, i= 8  j = 10\n",
      "i = 8  j = 11   hash = 49160  after, i= 8  j = 11\n",
      "i = 8  j = 12   hash = 53256  after, i= 8  j = 12\n",
      "i = 8  j = 13   hash = 57352  after, i= 8  j = 13\n",
      "i = 8  j = 14   hash = 61448  after, i= 8  j = 14\n",
      "i = 8  j = 15   hash = 65544  after, i= 8  j = 15\n",
      "i = 8  j = 16   hash = 69640  after, i= 8  j = 16\n",
      "i = 8  j = 17   hash = 73736  after, i= 8  j = 17\n",
      "i = 8  j = 18   hash = 77832  after, i= 8  j = 18\n",
      "i = 8  j = 19   hash = 81928  after, i= 8  j = 19\n",
      "i = 9  j = 10   hash = 45065  after, i= 9  j = 10\n",
      "i = 9  j = 11   hash = 49161  after, i= 9  j = 11\n",
      "i = 9  j = 12   hash = 53257  after, i= 9  j = 12\n",
      "i = 9  j = 13   hash = 57353  after, i= 9  j = 13\n",
      "i = 9  j = 14   hash = 61449  after, i= 9  j = 14\n",
      "i = 9  j = 15   hash = 65545  after, i= 9  j = 15\n",
      "i = 9  j = 16   hash = 69641  after, i= 9  j = 16\n",
      "i = 9  j = 17   hash = 73737  after, i= 9  j = 17\n",
      "i = 9  j = 18   hash = 77833  after, i= 9  j = 18\n",
      "i = 9  j = 19   hash = 81929  after, i= 9  j = 19\n",
      "i = 10  j = 11   hash = 49162  after, i= 10  j = 11\n",
      "i = 10  j = 12   hash = 53258  after, i= 10  j = 12\n",
      "i = 10  j = 13   hash = 57354  after, i= 10  j = 13\n",
      "i = 10  j = 14   hash = 61450  after, i= 10  j = 14\n",
      "i = 10  j = 15   hash = 65546  after, i= 10  j = 15\n",
      "i = 10  j = 16   hash = 69642  after, i= 10  j = 16\n",
      "i = 10  j = 17   hash = 73738  after, i= 10  j = 17\n",
      "i = 10  j = 18   hash = 77834  after, i= 10  j = 18\n",
      "i = 10  j = 19   hash = 81930  after, i= 10  j = 19\n",
      "i = 11  j = 12   hash = 53259  after, i= 11  j = 12\n",
      "i = 11  j = 13   hash = 57355  after, i= 11  j = 13\n",
      "i = 11  j = 14   hash = 61451  after, i= 11  j = 14\n",
      "i = 11  j = 15   hash = 65547  after, i= 11  j = 15\n",
      "i = 11  j = 16   hash = 69643  after, i= 11  j = 16\n",
      "i = 11  j = 17   hash = 73739  after, i= 11  j = 17\n",
      "i = 11  j = 18   hash = 77835  after, i= 11  j = 18\n",
      "i = 11  j = 19   hash = 81931  after, i= 11  j = 19\n",
      "i = 12  j = 13   hash = 57356  after, i= 12  j = 13\n",
      "i = 12  j = 14   hash = 61452  after, i= 12  j = 14\n",
      "i = 12  j = 15   hash = 65548  after, i= 12  j = 15\n",
      "i = 12  j = 16   hash = 69644  after, i= 12  j = 16\n",
      "i = 12  j = 17   hash = 73740  after, i= 12  j = 17\n",
      "i = 12  j = 18   hash = 77836  after, i= 12  j = 18\n",
      "i = 12  j = 19   hash = 81932  after, i= 12  j = 19\n",
      "i = 13  j = 14   hash = 61453  after, i= 13  j = 14\n",
      "i = 13  j = 15   hash = 65549  after, i= 13  j = 15\n",
      "i = 13  j = 16   hash = 69645  after, i= 13  j = 16\n",
      "i = 13  j = 17   hash = 73741  after, i= 13  j = 17\n",
      "i = 13  j = 18   hash = 77837  after, i= 13  j = 18\n",
      "i = 13  j = 19   hash = 81933  after, i= 13  j = 19\n",
      "i = 14  j = 15   hash = 65550  after, i= 14  j = 15\n",
      "i = 14  j = 16   hash = 69646  after, i= 14  j = 16\n",
      "i = 14  j = 17   hash = 73742  after, i= 14  j = 17\n",
      "i = 14  j = 18   hash = 77838  after, i= 14  j = 18\n",
      "i = 14  j = 19   hash = 81934  after, i= 14  j = 19\n",
      "i = 15  j = 16   hash = 69647  after, i= 15  j = 16\n",
      "i = 15  j = 17   hash = 73743  after, i= 15  j = 17\n",
      "i = 15  j = 18   hash = 77839  after, i= 15  j = 18\n",
      "i = 15  j = 19   hash = 81935  after, i= 15  j = 19\n",
      "i = 16  j = 17   hash = 73744  after, i= 16  j = 17\n",
      "i = 16  j = 18   hash = 77840  after, i= 16  j = 18\n",
      "i = 16  j = 19   hash = 81936  after, i= 16  j = 19\n",
      "i = 17  j = 18   hash = 77841  after, i= 17  j = 18\n",
      "i = 17  j = 19   hash = 81937  after, i= 17  j = 19\n",
      "i = 18  j = 19   hash = 81938  after, i= 18  j = 19\n"
     ]
    }
   ],
   "source": [
    "def hash_function(i, j, hash_key=4096):\n",
    "    return i + (j + 1) * hash_key\n",
    "\n",
    "def unhash(hash_val, hash_key=4096):\n",
    "    j = int(hash_val/hash_key) - 1\n",
    "    i = hash_val - (j + 1) * hash_key\n",
    "    return i, j\n",
    "\n",
    "## Put your code below. \n",
    "for i,j in combinations(range(ncolumns), 2):\n",
    "    hash_value = hash_function(i,j)\n",
    "    i_out, j_out = unhash(hash_value)\n",
    "    print('i = ' + str(i) + '  j = ' + str(j) + '   hash = ' + str(hash_value) + '  after, i= ' + str(i_out) + '  j = ' + str(j_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the results you have printed. Do the unhashed values of i and j agree with the original values? Is there any evidence of a **hash key collision** whereby two combinations of i and j hash to the same value? You can get a feel for the answer to the second question by noticing how the hash values change with i for a few fixed values of j.     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The map process\n",
    "\n",
    "We are constructing this example as a map-reduce algorithm. The first step is the map process that computes the t-test for the pairwise iid Normal vectors.   \n",
    "\n",
    "> **Exercise 1-2:** You will now create the code for the map task which computes the t-test results for every pair-wise combinations of the iid Normal vectors. By the following steps you will create code that represents a map task.  \n",
    "> 1. Create a loop over all combinations of the pairs of iid Normal vectors, i and j.  \n",
    "> 2. Filter out cases where $i = j$, the same vector.   \n",
    "> 3. Compute the hash key value for the indexes, i and j.  \n",
    "> 4. Add the values of the two-tailed t-statistic and p-value returned by the [tttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) function. The arguments to this function are the iid Normal vectors indexed by i and j. \n",
    "> 5. Once the loop has executed verify that that length of the dictionary is $(N^2 - N)/2$ as expected. This result verifies there have been no hash key collisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def map_hypothesis(vars):\n",
    "    ncolumns = vars.shape[1]\n",
    "    hypothesis_tests = {}\n",
    "    for i,j in combinations(range(ncolumns), 2): \n",
    "        if(i != j): # We only want to test between different samples \n",
    "            ## Compute the hash key\n",
    "            hash_key = hash_function(i,j)\n",
    "            hypothesis_tests[hash_key] = ttest_ind(vars[:,i], vars[:,j])\n",
    "    return hypothesis_tests\n",
    "\n",
    "hypothesis_tests = map_hypothesis(normal_vars)\n",
    "len(hypothesis_tests)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reduce task\n",
    "\n",
    "Now that the t-tests have been computed in the map process it is time to create a reduce process to filter the results to find the falsely significant results. The reduce task filters any non-significant cases from the dictionary. \n",
    "\n",
    "> **Exercise 1-3:** You will now create and apply the following code for the reduce process:   \n",
    "> 1. Create a loop over all combinations of the pairs of iid Normal vectors, i and j.  \n",
    "> 2. Filter out cases where $i = j$, where the keys will not exist in the dictionary.\n",
    "> 3. Compute the hash key with `hash_function()` defined above   \n",
    "> 4. Extract the t-statistic and p-value from the entry in the dictionary, indexed by the key.   \n",
    "> 5. If the p-value is greater than the significance cut-off level remove the dictionary entry with the `pop(key)` method. \n",
    "> 6. Once the loop has executed, print the length of the remaining dictionary and on one line the indexes of the iid Normal vectors, the t-statistic, and p-value of the significant tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significance_level = 0.05\n",
    "def reduce_significance(test_dictionary, significance_level):    \n",
    "    keys = list(test_dictionary.keys())\n",
    "    for key in keys: \n",
    "        t_statistic, p_value = test_dictionary[key] \n",
    "        if p_value > significance_level: test_dictionary.pop(key)\n",
    "    return test_dictionary                \n",
    "\n",
    "hypothesis_tests = reduce_significance(hypothesis_tests, significance_level)\n",
    "len(hypothesis_tests)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vectors i =  0, j =  1 the t-statistic = -3.2279  p-value = 0.0013\n",
      "For vectors i =  0, j =  2 the t-statistic = -2.2122  p-value = 0.0271\n",
      "For vectors i =  0, j =  3 the t-statistic = -2.3215  p-value = 0.0204\n",
      "For vectors i =  0, j =  4 the t-statistic = -3.3112  p-value = 0.0009\n",
      "For vectors i =  0, j =  5 the t-statistic = -2.8726  p-value = 0.0041\n",
      "For vectors i =  0, j =  6 the t-statistic = -2.6244  p-value = 0.0087\n",
      "For vectors i =  0, j =  7 the t-statistic = -2.6816  p-value = 0.0074\n",
      "For vectors i =  0, j =  8 the t-statistic = -3.7054  p-value = 0.0002\n",
      "For vectors i =  0, j = 10 the t-statistic = -2.4624  p-value = 0.0139\n",
      "For vectors i =  0, j = 12 the t-statistic = -2.0313  p-value = 0.0424\n",
      "For vectors i =  0, j = 13 the t-statistic = -2.9031  p-value = 0.0037\n",
      "For vectors i =  0, j = 14 the t-statistic = -2.2949  p-value = 0.0218\n",
      "For vectors i =  0, j = 15 the t-statistic = -2.2912  p-value = 0.0221\n",
      "For vectors i =  0, j = 16 the t-statistic = -3.0241  p-value = 0.0025\n",
      "For vectors i =  0, j = 17 the t-statistic = -3.9926  p-value = 0.0001\n",
      "For vectors i =  0, j = 18 the t-statistic = -2.1566  p-value = 0.0312\n",
      "For vectors i =  0, j = 19 the t-statistic = -2.2798  p-value = 0.0227\n",
      "For vectors i =  4, j = 11 the t-statistic = 2.0178  p-value = 0.0437\n",
      "For vectors i =  8, j =  9 the t-statistic = 1.9801  p-value = 0.0478\n",
      "For vectors i =  8, j = 11 the t-statistic = 2.4226  p-value = 0.0155\n",
      "For vectors i =  9, j = 17 the t-statistic = -2.2254  p-value = 0.0262\n",
      "For vectors i = 11, j = 17 the t-statistic = -2.6821  p-value = 0.0074\n"
     ]
    }
   ],
   "source": [
    "for key in hypothesis_tests.keys():\n",
    "    i, j = unhash(key)\n",
    "    print('For vectors i = {0:2d}, j = {1:2d} the t-statistic = {2:6.4f}  p-value = {3:6.4f}'.format(i, j, hypothesis_tests[key][0],hypothesis_tests[key][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the large number of apparently significant tests. Answer the following questions:  \n",
    "> 1. Is the number of false positive cases higher than expected?    \n",
    "> 2. Examine which of the iid Normal vectors contribute to the false positive results. Are there vectors which contribute multiple times?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni correction  \n",
    "\n",
    "Several adjustments to the multiple comparisons problem have been proposed. In Dunn published a method know as the **Bonfirroni correction** in 1961. The Bonferroni correction is a widely used method to reduce the false positive rate of hypothesis tests.  The adjustment is simple:\n",
    "$$\\alpha_b = \\frac{\\alpha}{m}\\\\\n",
    "with\\\\ \n",
    "m =\\ number\\ of\\ groups$$\n",
    "\n",
    "Can the Bonferroni correction help? Yes, by greatly increasing the confidence level required for a statistically significant result. The problem with the Bonfirroni correction is the reduction in power as the  grows smaller. For big data problems with large numbers of groups, this issue can be especially serious. \n",
    "\n",
    "**Exercise 1-4:** You will now apply the Bonferroni correction to the iid Normal vectors. To do so, :   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significance_bonforoni = significance_level/190.0\n",
    "hypothesis_tests = reduce_significance(hypothesis_tests, significance_bonforoni)\n",
    "len(hypothesis_tests)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even with the Bonforoni correction we have some false significance tests, if only just barely!    \n",
    "> **End of exercise.**\n",
    "\n",
    "But, can we detect small effect with Bonforoni correction, as this method significantly reduces power of tests? Execute the code in the cell below, which compares a standard Normal to a Normal with a small mean (effect size), to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002631578947368421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=array([-2.49553488]), pvalue=array([0.01265684]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.seed(567)\n",
    "print(significance_bonforoni)\n",
    "ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the Bonforoni correction, this difference in means would not be found significant. This illustrates the downside of the correction, which may prevent detection of significant effects, while still finding false significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate Control Methods   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(2334)\n",
    "normal_samples = nr.normal(size=(1000,ncolumns))\n",
    "normal_samples[:,:2] = np.add(normal_samples[:,:2], 0.1)\n",
    "hypothesis_tests = map_hypothesis(normal_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_tests = map_hypothesis(normal_vars)\n",
    "len(hypothesis_tests)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holm's method\n",
    "\n",
    "$$p(i) \\le Threshold(Holm's) = \\frac{\\alpha}{N - i + 1}$$\n",
    "\n",
    "Example: for the 10th ordered p-value with 1,000 total tests (genes) and significance level of 0.05, the cutoff is:   \n",
    "\n",
    "$$p(10) \\le \\frac{0.05}{1000 - 10 + 1} = 0.00005045$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_sort_key_reverse(kv_dictionary):  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020, 2021, Stephen F. Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
