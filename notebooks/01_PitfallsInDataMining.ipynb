{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01 - Pitfalls in Data Mining     \n",
    "## CSCI E-96\n",
    "\n",
    "The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences.   \n",
    "\n",
    "A related problem is cutting off a large-scale analysis when a desired relationship is 'found'. This practice of **p-value mining** often leads to unwarranted inferences.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hypothesis Tesing\n",
    "\n",
    "Testing multiple hypothesis in high-dimensional data can be problematic. Exhaustively testing all pairwise relationships between variables in a data set is a commonly used, but generally misleading from of **multiple comparisons**. The chance of finding false significance, using such a **data dredging** approach, can be surprisingly high. \n",
    "\n",
    "In this exercise you will perform multiple comparisons on only 20 **identically distributed independent (iid)** variables. Ideally, such tests should not find significant relationships, but the actual result is quite different. \n",
    "\n",
    "To get started, execute the code in the cell below to load the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from itertools import product, combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will apply a t-test to all pairwise combinations of identical Normally distributed variables. In this case, we will create a data set with 20 iid Normal distributions of 1000 samples each. Execute the code in the cell below to find this data and display the mean and variance of each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The means of the columns are\n",
      " [-1.16191649e-01  2.80829317e-02 -1.78516419e-02 -1.44691489e-02\n",
      "  3.03718152e-02  1.20007442e-02 -9.58845606e-05  1.98662580e-03\n",
      "  4.94154934e-02 -4.11640866e-02 -6.32977862e-03 -5.93868192e-02\n",
      " -2.56373595e-02  1.43568791e-02 -1.44725765e-02 -1.37023955e-02\n",
      "  1.80622439e-02  5.87029691e-02 -2.02650514e-02 -1.56346106e-02]\n",
      "\n",
      "The variances of the columns are\n",
      " [0.94834508 1.04744241 1.0258018  0.96977571 1.0089001  1.04113864\n",
      " 1.00657222 0.99192594 1.04713487 1.04329434 1.04023108 0.96791346\n",
      " 1.03706907 1.07179865 1.01431404 1.05060289 1.02054329 0.9686211\n",
      " 1.02810287 0.99521555]\n"
     ]
    }
   ],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(234)\n",
    "normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "print('The means of the columns are\\n', np.mean(normal_vars, axis = 0))\n",
    "print('\\nThe variances of the columns are\\n', np.var(normal_vars, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that means and variances are close to 0.0 and 1.0 respectively. As expected, there is not much difference between these variables.\n",
    "\n",
    "How many of these t-tests will show **significance** at the 0.05 cut-off level? There are 380 pairwise combinations, so we expect to find a number of falsely significant test results at this level. To find out, complete and execute the code in the cell below to filter the test results and print those that show significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a hash \n",
    "\n",
    "The goal of this exercise is to compute pairwise hypothesis tests of the differences in means for each of the iid Normal vectors. As an intermediate step you will create a dictionary with the results of these hypothesis tests. The dictionaries store **key-value**, $(K,V)$, pairs. Each key must represent an index for the two vectors used to compute the test statistic. \n",
    "\n",
    "The question is, how can we represent the key for the pair of vectors? One option is to use a dictionary of dictionaries. This approach has a dictionary indexed by the first key, which contains a dictionary indexed by the second key. While, this nested dictionary approach would work, it requires two key look-ups per value. A better approach is to create a hash of the two indexes and use the hash as the key for the dictionary. Using a hash the values in the dictionary can be accessed in a single step with using hashed key.\n",
    "\n",
    "\n",
    "> **Computational Note:** The Python dictionary is an efficient and reasonably scalable **hash table**. The hash function used depends on the type of the key; integer, string, etc. The resulting dictionary of key-value pairs, $(K,V)$, can therefore be access in far less than linear time, often about $O(log(N))$.  \n",
    "\n",
    "If you are not familiar with Python dictionaries you can find a short tutorial [here](https://www.tutorialspoint.com/python_data_structure/python_hash_table.htm), as well as many other places on the web.\n",
    "\n",
    "> **Exercise 1-1:** Given that our space of vectors is actually quite small, just 20, we do not need a sophisticated and scalable hash function. This hashed key will then be used to store and retrieve the values using a Python dictionary, in about $O(log(N))$ time.     \n",
    "\n",
    "> In this exercise you will test a simple hash function and its inverse. Examine the code below and notice that the hash function encodes the two indexes into a single integer by simple additional and multiplication. Division (a slower process) is avoided. Efficiency of the inverse hash function is less important, since it is used less frequently.  \n",
    "\n",
    "> To test this hash, do the following:    \n",
    "> 1. Using the Python [ittertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations) function create all unique pairwise combinations of indexes i and j. The arguments to this function are the indexes to the iid Normal vectors. The iterator is `range(ncolumns)`.    \n",
    "> 2. Within this loop compute the hash, and the inverse hash of the indexes, i, and j.   \n",
    "> 3. On a single line print the following; the values of i and j, the hash key value, and the unhashed values of i and j.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0  j = 1   hash = 2048\n",
      "i = 0  j = 2   hash = 3072\n",
      "i = 0  j = 3   hash = 4096\n",
      "i = 0  j = 4   hash = 5120\n",
      "i = 0  j = 5   hash = 6144\n",
      "i = 0  j = 6   hash = 7168\n",
      "i = 0  j = 7   hash = 8192\n",
      "i = 0  j = 8   hash = 9216\n",
      "i = 0  j = 9   hash = 10240\n",
      "i = 0  j = 10   hash = 11264\n",
      "i = 0  j = 11   hash = 12288\n",
      "i = 0  j = 12   hash = 13312\n",
      "i = 0  j = 13   hash = 14336\n",
      "i = 0  j = 14   hash = 15360\n",
      "i = 0  j = 15   hash = 16384\n",
      "i = 0  j = 16   hash = 17408\n",
      "i = 0  j = 17   hash = 18432\n",
      "i = 0  j = 18   hash = 19456\n",
      "i = 0  j = 19   hash = 20480\n",
      "i = 1  j = 2   hash = 3073\n",
      "i = 1  j = 3   hash = 4097\n",
      "i = 1  j = 4   hash = 5121\n",
      "i = 1  j = 5   hash = 6145\n",
      "i = 1  j = 6   hash = 7169\n",
      "i = 1  j = 7   hash = 8193\n",
      "i = 1  j = 8   hash = 9217\n",
      "i = 1  j = 9   hash = 10241\n",
      "i = 1  j = 10   hash = 11265\n",
      "i = 1  j = 11   hash = 12289\n",
      "i = 1  j = 12   hash = 13313\n",
      "i = 1  j = 13   hash = 14337\n",
      "i = 1  j = 14   hash = 15361\n",
      "i = 1  j = 15   hash = 16385\n",
      "i = 1  j = 16   hash = 17409\n",
      "i = 1  j = 17   hash = 18433\n",
      "i = 1  j = 18   hash = 19457\n",
      "i = 1  j = 19   hash = 20481\n",
      "i = 2  j = 3   hash = 4098\n",
      "i = 2  j = 4   hash = 5122\n",
      "i = 2  j = 5   hash = 6146\n",
      "i = 2  j = 6   hash = 7170\n",
      "i = 2  j = 7   hash = 8194\n",
      "i = 2  j = 8   hash = 9218\n",
      "i = 2  j = 9   hash = 10242\n",
      "i = 2  j = 10   hash = 11266\n",
      "i = 2  j = 11   hash = 12290\n",
      "i = 2  j = 12   hash = 13314\n",
      "i = 2  j = 13   hash = 14338\n",
      "i = 2  j = 14   hash = 15362\n",
      "i = 2  j = 15   hash = 16386\n",
      "i = 2  j = 16   hash = 17410\n",
      "i = 2  j = 17   hash = 18434\n",
      "i = 2  j = 18   hash = 19458\n",
      "i = 2  j = 19   hash = 20482\n",
      "i = 3  j = 4   hash = 5123\n",
      "i = 3  j = 5   hash = 6147\n",
      "i = 3  j = 6   hash = 7171\n",
      "i = 3  j = 7   hash = 8195\n",
      "i = 3  j = 8   hash = 9219\n",
      "i = 3  j = 9   hash = 10243\n",
      "i = 3  j = 10   hash = 11267\n",
      "i = 3  j = 11   hash = 12291\n",
      "i = 3  j = 12   hash = 13315\n",
      "i = 3  j = 13   hash = 14339\n",
      "i = 3  j = 14   hash = 15363\n",
      "i = 3  j = 15   hash = 16387\n",
      "i = 3  j = 16   hash = 17411\n",
      "i = 3  j = 17   hash = 18435\n",
      "i = 3  j = 18   hash = 19459\n",
      "i = 3  j = 19   hash = 20483\n",
      "i = 4  j = 5   hash = 6148\n",
      "i = 4  j = 6   hash = 7172\n",
      "i = 4  j = 7   hash = 8196\n",
      "i = 4  j = 8   hash = 9220\n",
      "i = 4  j = 9   hash = 10244\n",
      "i = 4  j = 10   hash = 11268\n",
      "i = 4  j = 11   hash = 12292\n",
      "i = 4  j = 12   hash = 13316\n",
      "i = 4  j = 13   hash = 14340\n",
      "i = 4  j = 14   hash = 15364\n",
      "i = 4  j = 15   hash = 16388\n",
      "i = 4  j = 16   hash = 17412\n",
      "i = 4  j = 17   hash = 18436\n",
      "i = 4  j = 18   hash = 19460\n",
      "i = 4  j = 19   hash = 20484\n",
      "i = 5  j = 6   hash = 7173\n",
      "i = 5  j = 7   hash = 8197\n",
      "i = 5  j = 8   hash = 9221\n",
      "i = 5  j = 9   hash = 10245\n",
      "i = 5  j = 10   hash = 11269\n",
      "i = 5  j = 11   hash = 12293\n",
      "i = 5  j = 12   hash = 13317\n",
      "i = 5  j = 13   hash = 14341\n",
      "i = 5  j = 14   hash = 15365\n",
      "i = 5  j = 15   hash = 16389\n",
      "i = 5  j = 16   hash = 17413\n",
      "i = 5  j = 17   hash = 18437\n",
      "i = 5  j = 18   hash = 19461\n",
      "i = 5  j = 19   hash = 20485\n",
      "i = 6  j = 7   hash = 8198\n",
      "i = 6  j = 8   hash = 9222\n",
      "i = 6  j = 9   hash = 10246\n",
      "i = 6  j = 10   hash = 11270\n",
      "i = 6  j = 11   hash = 12294\n",
      "i = 6  j = 12   hash = 13318\n",
      "i = 6  j = 13   hash = 14342\n",
      "i = 6  j = 14   hash = 15366\n",
      "i = 6  j = 15   hash = 16390\n",
      "i = 6  j = 16   hash = 17414\n",
      "i = 6  j = 17   hash = 18438\n",
      "i = 6  j = 18   hash = 19462\n",
      "i = 6  j = 19   hash = 20486\n",
      "i = 7  j = 8   hash = 9223\n",
      "i = 7  j = 9   hash = 10247\n",
      "i = 7  j = 10   hash = 11271\n",
      "i = 7  j = 11   hash = 12295\n",
      "i = 7  j = 12   hash = 13319\n",
      "i = 7  j = 13   hash = 14343\n",
      "i = 7  j = 14   hash = 15367\n",
      "i = 7  j = 15   hash = 16391\n",
      "i = 7  j = 16   hash = 17415\n",
      "i = 7  j = 17   hash = 18439\n",
      "i = 7  j = 18   hash = 19463\n",
      "i = 7  j = 19   hash = 20487\n",
      "i = 8  j = 9   hash = 10248\n",
      "i = 8  j = 10   hash = 11272\n",
      "i = 8  j = 11   hash = 12296\n",
      "i = 8  j = 12   hash = 13320\n",
      "i = 8  j = 13   hash = 14344\n",
      "i = 8  j = 14   hash = 15368\n",
      "i = 8  j = 15   hash = 16392\n",
      "i = 8  j = 16   hash = 17416\n",
      "i = 8  j = 17   hash = 18440\n",
      "i = 8  j = 18   hash = 19464\n",
      "i = 8  j = 19   hash = 20488\n",
      "i = 9  j = 10   hash = 11273\n",
      "i = 9  j = 11   hash = 12297\n",
      "i = 9  j = 12   hash = 13321\n",
      "i = 9  j = 13   hash = 14345\n",
      "i = 9  j = 14   hash = 15369\n",
      "i = 9  j = 15   hash = 16393\n",
      "i = 9  j = 16   hash = 17417\n",
      "i = 9  j = 17   hash = 18441\n",
      "i = 9  j = 18   hash = 19465\n",
      "i = 9  j = 19   hash = 20489\n",
      "i = 10  j = 11   hash = 12298\n",
      "i = 10  j = 12   hash = 13322\n",
      "i = 10  j = 13   hash = 14346\n",
      "i = 10  j = 14   hash = 15370\n",
      "i = 10  j = 15   hash = 16394\n",
      "i = 10  j = 16   hash = 17418\n",
      "i = 10  j = 17   hash = 18442\n",
      "i = 10  j = 18   hash = 19466\n",
      "i = 10  j = 19   hash = 20490\n",
      "i = 11  j = 12   hash = 13323\n",
      "i = 11  j = 13   hash = 14347\n",
      "i = 11  j = 14   hash = 15371\n",
      "i = 11  j = 15   hash = 16395\n",
      "i = 11  j = 16   hash = 17419\n",
      "i = 11  j = 17   hash = 18443\n",
      "i = 11  j = 18   hash = 19467\n",
      "i = 11  j = 19   hash = 20491\n",
      "i = 12  j = 13   hash = 14348\n",
      "i = 12  j = 14   hash = 15372\n",
      "i = 12  j = 15   hash = 16396\n",
      "i = 12  j = 16   hash = 17420\n",
      "i = 12  j = 17   hash = 18444\n",
      "i = 12  j = 18   hash = 19468\n",
      "i = 12  j = 19   hash = 20492\n",
      "i = 13  j = 14   hash = 15373\n",
      "i = 13  j = 15   hash = 16397\n",
      "i = 13  j = 16   hash = 17421\n",
      "i = 13  j = 17   hash = 18445\n",
      "i = 13  j = 18   hash = 19469\n",
      "i = 13  j = 19   hash = 20493\n",
      "i = 14  j = 15   hash = 16398\n",
      "i = 14  j = 16   hash = 17422\n",
      "i = 14  j = 17   hash = 18446\n",
      "i = 14  j = 18   hash = 19470\n",
      "i = 14  j = 19   hash = 20494\n",
      "i = 15  j = 16   hash = 17423\n",
      "i = 15  j = 17   hash = 18447\n",
      "i = 15  j = 18   hash = 19471\n",
      "i = 15  j = 19   hash = 20495\n",
      "i = 16  j = 17   hash = 18448\n",
      "i = 16  j = 18   hash = 19472\n",
      "i = 16  j = 19   hash = 20496\n",
      "i = 17  j = 18   hash = 19473\n",
      "i = 17  j = 19   hash = 20497\n",
      "i = 18  j = 19   hash = 20498\n"
     ]
    }
   ],
   "source": [
    "def hash_function(i, j, hash_key=1024, modulo_multiplier=32):\n",
    "    modulo = hash_key * modulo_multiplier\n",
    "    return (i + (j + 1) * hash_key) % modulo\n",
    "\n",
    "## Put your code below. \n",
    "for i,j in combinations(range(ncolumns), 2):\n",
    "    hash = hash_function(i,j)\n",
    "    print('i = ' + str(i) + '  j = ' + str(j) + '   hash = ' + str(hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "190\n"
     ]
    }
   ],
   "source": [
    "hash_list = [hash_function(i,j) for i,j in combinations(range(ncolumns), 2)]\n",
    "print(len(hash_list))\n",
    "print(len(np.unique(hash_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the results you have printed. Do the unhashed values of i and j agree with the original values? Is there any evidence of a **hash key collision** whereby two combinations of i and j hash to the same value? You can get a feel for the answer to the second question by noticing how the hash values change with i for a few fixed values of j.     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The map process\n",
    "\n",
    "We are constructing this example as a map-reduce algorithm. The first step is the map process that computes the t-test for the pairwise iid Normal vectors.   \n",
    "\n",
    "> **Exercise 1-2:** You will now create the code for the map task which computes the t-test results for every pair-wise combinations of the iid Normal vectors. By the following steps you will create code that represents a map task.  \n",
    "> 1. Create a loop over all combinations of the pairs of iid Normal vectors, i and j.  \n",
    "> 2. Filter out cases where $i = j$, the same vector.   \n",
    "> 3. Compute the hash key value for the indexes, i and j.  \n",
    "> 4. Add the values of the two-tailed t-statistic and p-value returned by the [tttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) function. The arguments to this function are the iid Normal vectors indexed by i and j. \n",
    "> 5. Once the loop has executed verify that that length of the dictionary is $(N^2 - N)/2$ as expected. This result verifies there have been no hash key collisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the hash table =  32768\n"
     ]
    }
   ],
   "source": [
    "def map_hypothesis(vars, hash_key=1024, modulo_multiplier=32):\n",
    "    ncolumns = vars.shape[1]\n",
    "    nhash = hash_key * modulo_multiplier \n",
    "    hash_table = pd.DataFrame({'i':[np.nan]*nhash, 'j':[np.nan]*nhash})\n",
    "    for i,j in combinations(range(ncolumns), 2): \n",
    "        ## Compute the hash key\n",
    "        hash_key = hash_function(i,j)\n",
    "        hash_table.iloc[hash_key,:] = [i,j]\n",
    "    return hash_table\n",
    "\n",
    "hash_table = map_hypothesis(normal_vars)\n",
    "print('length of the hash table = ', str(len(hash_table)))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shuffle and reduce task\n",
    "\n",
    "Now that the t-tests have been computed in the map process it is time to create a reduce process to filter the results to find the falsely significant results. The reduce task filters any non-significant cases from the dictionary. \n",
    "\n",
    "> **Exercise 1-3:** You will now create and apply the following code for the reduce process:   \n",
    "> 1. Create a loop over all combinations of the pairs of iid Normal vectors, i and j.  \n",
    "> 2. Filter out cases where $i = j$, where the keys will not exist in the dictionary.\n",
    "> 3. Compute the hash key with `hash_function()` defined above   \n",
    "> 4. Extract the t-statistic and p-value from the entry in the dictionary, indexed by the key.   \n",
    "> 5. If the p-value is greater than the significance cut-off level remove the dictionary entry with the `pop(key)` method. \n",
    "> 6. Once the loop has executed, print the length of the remaining dictionary and on one line the indexes of the iid Normal vectors, the t-statistic, and p-value of the significant tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "def reduce_significance(hash_table, values, significance_level):  \n",
    "#    hash_table['p_value'] = [np.nan] * hash_table.shape[0]\n",
    "#    hash_table['p_value'] = [np.nan] * hash_table.shape[0]\n",
    "    ## As a substitute for a shuffle we will use a simple search \n",
    "    ## through the data frame  \n",
    "    for hash in range(hash_table.shape[0]): \n",
    "        if not np.isnan(hash_table.iloc[hash,0]):\n",
    "            i = int(hash_table.iloc[hash,0]) \n",
    "            j = int(hash_table.iloc[hash,1])\n",
    "    ## Given the i,j pair we need to compute the t-statistic and the p-value        \n",
    "        \n",
    "\n",
    "reduce_significance(hash_table, normal_vars, significance_level)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For vectors i =  0, j =  1 the t-statistic = -3.9926  p-value = 0.0001\n",
      "For vectors i =  0, j =  2 the t-statistic = -2.1566  p-value = 0.0312\n",
      "For vectors i =  0, j = -1 the t-statistic = -2.2798  p-value = 0.0227\n",
      "For vectors i =  0, j =  0 the t-statistic = -3.0241  p-value = 0.0025\n",
      "For vectors i =  9, j =  1 the t-statistic = -2.2254  p-value = 0.0262\n",
      "For vectors i = 11, j =  1 the t-statistic = -2.6821  p-value = 0.0074\n"
     ]
    }
   ],
   "source": [
    "for key in hypothesis_tests.keys():\n",
    "    i, j = unhash(key)\n",
    "    print('For vectors i = {0:2d}, j = {1:2d} the t-statistic = {2:6.4f}  p-value = {3:6.4f}'.format(i, j, hypothesis_tests[key][0],hypothesis_tests[key][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the large number of apparently significant tests. Answer the following questions:  \n",
    "> 1. Is the number of false positive cases higher than expected?    \n",
    "> 2. Examine which of the iid Normal vectors contribute to the false positive results. Are there vectors which contribute multiple times?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni correction  \n",
    "\n",
    "Several adjustments to the multiple comparisons problem have been proposed. In Dunn published a method know as the **Bonfirroni correction** in 1961. The Bonferroni correction is a widely used method to reduce the false positive rate of hypothesis tests.  The adjustment is simple:\n",
    "$$\\alpha_b = \\frac{\\alpha}{m}\\\\\n",
    "with\\\\ \n",
    "m =\\ number\\ of\\ groups$$\n",
    "\n",
    "Can the Bonferroni correction help? Yes, by greatly increasing the confidence level required for a statistically significant result. The problem with the Bonfirroni correction is the reduction in power as the  grows smaller. For big data problems with large numbers of groups, this issue can be especially serious. \n",
    "\n",
    "**Exercise 1-4:** You will now apply the Bonferroni correction to the iid Normal vectors. To do so, :   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significance_bonforoni = significance_level/190.0\n",
    "hypothesis_tests = reduce_significance(hypothesis_tests, significance_bonforoni)\n",
    "len(hypothesis_tests)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even with the Bonforoni correction we have some false significance tests, if only just barely!    \n",
    "> **End of exercise.**\n",
    "\n",
    "But, can we detect small effect with Bonforoni correction, as this method significantly reduces power of tests? Execute the code in the cell below, which compares a standard Normal to a Normal with a small mean (effect size), to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002631578947368421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=array([-2.49553488]), pvalue=array([0.01265684]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr.seed(567)\n",
    "print(significance_bonforoni)\n",
    "ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the Bonforoni correction, this difference in means would not be found significant. This illustrates the downside of the correction, which may prevent detection of significant effects, while still finding false significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate Control Methods   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(2334)\n",
    "normal_samples = nr.normal(size=(1000,ncolumns))\n",
    "normal_samples[:,:2] = np.add(normal_samples[:,:2], 0.1)\n",
    "hypothesis_tests = map_hypothesis(normal_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_tests = map_hypothesis(normal_vars)\n",
    "len(hypothesis_tests)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holm's method\n",
    "\n",
    "$$p(i) \\le Threshold(Holm's) = \\frac{\\alpha}{N - i + 1}$$\n",
    "\n",
    "Example: for the 10th ordered p-value with 1,000 total tests (genes) and significance level of 0.05, the cutoff is:   \n",
    "\n",
    "$$p(10) \\le \\frac{0.05}{1000 - 10 + 1} = 0.00005045$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-11-3452be0b91d9>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-3452be0b91d9>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def reduce_sort_key_reverse(kv_dictionary):  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2020, 2021, Stephen F. Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
